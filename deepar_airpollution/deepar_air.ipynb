{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker で提供される DeepAR アルゴリズムの学習と推論を行う\n",
    "\n",
    "#### ノートブックに含まれる内容\n",
    "\n",
    "- Amazon が提供するアルゴリズムの使いかた\n",
    "- 中でも，DeepAR アルゴリズムの使い方\n",
    "\n",
    "#### ノートブックで使われている手法の詳細\n",
    "\n",
    "- アルゴリズム: DeepAR\n",
    "- データ: ランダム時系列を作成\n",
    "- 可視化手段: matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep AR とは\n",
    "\n",
    "DeepAR とは，AWS から提供されている state-of-the-art の時系列データ予測アルゴリズムです．このノートブックでは，DeepAR を使って，モデルの学習と推論を行います．DeepAR は，スカラの時系列データに対する予測を行う，教師あり学習アルゴリズムです．古典的な ARIMA モデルや指数平滑法は，単一時系列のみを学習に用いており，そのためしばしば極端な予測値を出力します．DeepAR は複数の時系列を入力として取り，併せて学習することで，より安定した予測結果を出すことができます．そのため，例えば多種の製品の需要，サーバ負荷，Webページへのリクエスト数といった複数の時系列が手に入るような場合に，非常に有効です．\n",
    "\n",
    "DeepAR の詳細については，[公式ドキュメントの解説](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html)および [arXiv の論文](https://arxiv.org/abs/1704.04110)をご覧ください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは SageMaker SDK を使って作業を行います．また，学習データを S3 にアップロードする際に，s3fs モジュールを使用します．当該モジュールがインスタンスに入っていないため，ここでは conda コマンドでインストールします（もちろん，その他のモジュールも `pip` コマンドでインストールすることができます）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを置くバケットを指定します．このバケットは，ノートブックインスタンスがある（そして，学習と推論を行う）リージョンと同一である必要があります．また SageMaker セッションで使用する IAM role に，データに対するアクセス権限を与える必要があります．ここでは `get_execution_role` を使って，ノートブックインスタンス作成時に割り当てたロールを使用します．\n",
    "\n",
    "以下の**<span style=\"color: red;\"> 1 行目のバケット名について，`sagemaker-ap-northeast-1-handson-YYYYMMDD-XX` の `YYYYMMDD` を今日の日付に，`XX`にはIAMユーザ名などをいれて一意な名前となるようにしてください</span>**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "bucket = 'sagemaker-ap-northeast-1-handson-YYYYMMDD-XX'\n",
    "prefix = 'sagemaker/DEMO-deepar'\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "boto3.resource('s3').create_bucket(Bucket=bucket, CreateBucketConfiguration={'LocationConstraint': 'ap-northeast-1'})\n",
    "s3_data_path = \"{}/{}/data\".format(bucket, prefix)\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，当該リージョンで提供されているコンテナイメージを取得します．AWS から提供されているイメージについては，[公式ドキュメントに一覧](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)があります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {\n",
    "    'us-east-1': '522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:latest',\n",
    "    'us-east-2': '566113047672.dkr.ecr.us-east-2.amazonaws.com/forecasting-deepar:latest',\n",
    "    'us-west-2': '156387875391.dkr.ecr.us-west-2.amazonaws.com/forecasting-deepar:latest',\n",
    "    'eu-west-1': '224300973850.dkr.ecr.eu-west-1.amazonaws.com/forecasting-deepar:latest',\n",
    "    'ap-northeast-1': '633353088612.dkr.ecr.ap-northeast-1.amazonaws.com/forecasting-deepar:latest',\n",
    "}\n",
    "image_name = containers[boto3.Session().region_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットのダウンロード\n",
    "データセットをダウンロードしてnotebookインスタンスと同じ場所に置きます。手元のクライアントPCにダウンロードしてからJupyterにアップロードすることも可能ですが、直接、notebookインスタンスの場所にダウンロードすることができます。何かをダウンロードしたいときは  \n",
    "```\n",
    "!wget (ダウンロードしたいファイルのURL)\n",
    "```\n",
    "とします。`wget`はファイルをダウンロードするコマンドで、冒頭に`!`をつけるとJupyter上で実行することができます。ここでは`data`というフォルダを作って、そこにダウンロードしましょう。以下のセルを選択して実行（ctrl+Enter、shift+Enterまたは、上のメニューからRunをクリック）してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!cd data && wget https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットをのぞいてみる\n",
    "データセットにどういうデータが含まれていて、それらがどういう風に表現されているのかを確認してみましょう。データの可視化や処理には`pandas`を使うのが便利です。例えば以下のことに気がつくと思います。\n",
    "- pm2.5には観測値がない (NaN）ところがあるので、これらをそのまま使えない。\n",
    "- 風向きはカテゴリ変数なのでダミー変数に置き換える必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "data = pd.read_csv('./data/PRSA_data_2010.1.1-2014.12.31.csv')\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理を実行する\n",
    "\n",
    "ここでは以下のような前処理を実行します。\n",
    "- 年〜時までの時間に関するデータをマージしてindexにする。\n",
    "- カテゴリ変数をone-hot encodingする\n",
    "- 不要になった列を消す（Noや時間に関する列など）\n",
    "- NaNを含む行を削除し、線形補間する。\n",
    "\n",
    "最終的なレコード数と特徴数を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "data['time'] = data.apply(lambda x: datetime(\n",
    "                          x['year'], x['month'], x['day'], x['hour']), axis=1)\n",
    "data.set_index('time', inplace = True) \n",
    "\n",
    "# Categoriacal variable is converted into dummy variables.\n",
    "cbwd_dummy = pd.get_dummies(data['cbwd'])\n",
    "data = pd.concat([data, cbwd_dummy], axis = 1)\n",
    "\n",
    "# Unnecessary variables are dropped here.\n",
    "data.drop(columns=['No', 'cbwd', 'year', 'month', 'day', 'hour'], inplace =True)\n",
    "\n",
    "# Rows including n/a are dropped.\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Linear interploation\n",
    "data = data.resample('h').interpolate(method='linear')\n",
    "\n",
    "print(\"Rows: {}, Columns: {}\".format(len(data.index), len(data.columns)))\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習/テストデータの分割\n",
    "\n",
    "学習/テストデータの分割については2013年までを学習データ、2014年をトレーニングデータとします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[data.index.year <2014]\n",
    "test_data = data[data.index.year ==2014]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データの最初の100データのpm2.5だけ表示してみます。（pm2.5のindexは0です）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_data.index[:100], train_data.values[:100, 0], label=\"pm2.5\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas.Series` フォーマットの時系列データを[Deep AR で指定している JSON 形式](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html)に変換して，S3 にアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_obj(ts, cat):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": list(ts)}\n",
    "    obj[\"cat\"] = cat\n",
    "    return obj\n",
    "\n",
    "def series_to_jsonline(ts, cat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoding = \"utf-8\"\n",
    "s3filesystem = s3fs.S3FileSystem()\n",
    "\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'wb') as fp:\n",
    "        for i in range(len(train_data.columns)):\n",
    "            ts = train_data.iloc[:, i]\n",
    "            fp.write(series_to_jsonline(ts, i).encode(encoding))\n",
    "            fp.write('\\n'.encode(encoding))\n",
    "\n",
    "\n",
    "with s3filesystem.open(s3_data_path + \"/test/test.json\", 'wb') as fp:\n",
    "        for i in range(len(test_data.columns)):\n",
    "            ts = test_data.iloc[:, i]\n",
    "            fp.write(series_to_jsonline(ts, i).encode(encoding))\n",
    "            fp.write('\\n'.encode(encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず，`Estimator` オブジェクトを作成します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.xlarge',\n",
    "    base_job_name='DEMO-deepar',\n",
    "    output_path=\"s3://\" + s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成した `Estimator` に対して，ハイパーパラメタをセットします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": 'H',\n",
    "    \"context_length\": 72,\n",
    "    \"prediction_length\": 12,\n",
    "    \"num_cells\": \"40\",\n",
    "    \"num_layers\": \"3\",\n",
    "    \"likelihood\": \"gaussian\",\n",
    "    \"epochs\": \"20\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"dropout_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\",\n",
    "    \"embedding_dimension\": \"10\",\n",
    "    \"cardinality\": len(train_data.columns)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習に使用するデータを指定して，ジョブを開始します．この際に `test` データチャネルを指定することで，学習済みモデルをテストデータに適用した際の，accuracy を出力してくれます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    \"train\": \"s3://{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの推論を実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.lat est_training_job.name\n",
    "\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    deployment_image=image_name,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論の際に，`pandas.Series` オブジェクトを入力として渡せるようにするための，ラッパークラスを定義します．本来は DeepAR で指定された JSON フォーマットなためです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "\n",
    "    def set_prediction_parameters(self, freq, prediction_length):\n",
    "        \"\"\"Set the time frequency and prediction length parameters. This method **must** be called\n",
    "        before being able to use `predict`.\n",
    "        \n",
    "        Parameters:\n",
    "        freq -- string indicating the time frequency\n",
    "        prediction_length -- integer, number of predicted time points\n",
    "        \n",
    "        Return value: none.\n",
    "        \"\"\"\n",
    "        self.freq = freq\n",
    "        self.prediction_length = prediction_length\n",
    "        \n",
    "    def predict(self, ts, cat=None, encoding=\"utf-8\", num_samples=100, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        Parameters:\n",
    "        ts -- list of `pandas.Series` objects, the time series to predict\n",
    "        cat -- list of integers (default: None)\n",
    "        encoding -- string, encoding to use for the request (default: \"utf-8\")\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_times = [ts.iloc[:, i].index[-1]+1 for  i in range(len(ts.columns))]\n",
    "        req = self.__encode_request(ts, cat, encoding, num_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, prediction_times, encoding)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, encoding, num_samples, quantiles):\n",
    "        instances = [series_to_obj(ts.iloc[:,k], k) for k in range(len(ts.columns))]\n",
    "        configuration = {\"num_samples\": num_samples, \"output_types\": [\"quantiles\"], \"quantiles\": quantiles}\n",
    "        http_request_data = {\"instances\": instances, \"configuration\": configuration}\n",
    "        return json.dumps(http_request_data).encode(encoding)\n",
    "    \n",
    "    def __decode_response(self, response, prediction_times, encoding):\n",
    "        response_data = json.loads(response.decode(encoding))\n",
    "        list_of_df = []\n",
    "        for k in range(len(prediction_times)):\n",
    "            prediction_index = pd.DatetimeIndex(start=prediction_times[k], freq=self.freq, periods=self.prediction_length)\n",
    "            list_of_df.append(pd.DataFrame(data=response_data['predictions'][k]['quantiles'], index=prediction_index))\n",
    "        return list_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = DeepARPredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "freq = 'H'\n",
    "predictor.set_prediction_parameters(freq, prediction_length=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に予測を行って結果をグラフに表示します．今回は2010-2013年の学習データを入れて、2014年の最初の12時間の予測を行います。予測対象はpm2.5の値のみです。同時刻の実データをtest_dataから抽出して比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練に使った２０１０年から2013年末のデータを入れる\n",
    "# テストに含まれる2014年の実測値のみを比較する。\n",
    "#print(train_data)\n",
    "list_of_df = predictor.predict(train_data)\n",
    "#print(list_of_df)\n",
    "predict = list_of_df[0]\n",
    "predict2014 = predict[predict.index.year == 2014]\n",
    "actual2014 = test_data.loc[predict2014.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "p10 = predict2014['0.1']\n",
    "p90 = predict2014['0.9']\n",
    "plt.fill_between(p10.index, p10, p90, color='y', alpha=0.5, label='80% confidence interval')\n",
    "plt.plot(p10.index, predict2014['0.5'], label='prediction median')\n",
    "plt.plot(p10.index, actual2014['pm2.5'], label='ground truth')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に2014年のテストデータを入れて1時間先のデータを逐次予測していきます。合計24時間のデータを予測します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=0\n",
    "end_time = 24\n",
    "one_step_predict = pd.DataFrame()\n",
    "one_step_time = []\n",
    "for i in range(start_time, end_time):\n",
    "    print(\"\\r prediction: {}/{} done\".format(i+1 - start_time, end_time-start_time), end=\"\")\n",
    "    list_of_df = predictor.predict(test_data.iloc[i: i+72])\n",
    "    one_step_predict = pd.concat([one_step_predict, list_of_df[0].iloc[0:1]])\n",
    "\n",
    "one_step_predict2014 = one_step_predict\n",
    "one_step_actual2014 = test_data.loc[one_step_predict.index]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "p10 = one_step_predict2014['0.1']\n",
    "p90 = one_step_predict2014['0.9']\n",
    "plt.fill_between(p10.index, p10, p90, color='y', alpha=0.5, label='80% confidence interval')\n",
    "plt.plot(p10.index, one_step_predict2014['0.5'], label='prediction median')\n",
    "plt.plot(p10.index, one_step_actual2014['pm2.5'], label='ground truth')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの削除\n",
    "\n",
    "全て終わったら，エンドポイントを削除します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
