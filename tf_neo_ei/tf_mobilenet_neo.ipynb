{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow の学習済みモデルを学習に利用する\n",
    "### 学習済みモデルのダウンロード\n",
    "学習済みモデルがS3にアップロードされていれば、学習インスタンスで読み込んで、学習済みモデルから学習をスタートすることできます。\n",
    "まずは、tensorflowのv1_0.25_128のモデルをダウンロード・解凍します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "download_file = \"mobilenet_v1_0.25_128.tgz\"\n",
    "if not os.path.exists(download_file):\n",
    "    url =\"http://download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_0.25_128.tgz\"\n",
    "    urllib.request.urlretrieve(url, download_file)\n",
    "    \n",
    "!tar xvzf $download_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのダウンロード\n",
    "\n",
    "### Validation dataのGround Truthのダウンロード\n",
    "\n",
    "Tensorflowの学習済みモデルが出力するラベルIDは、ILSVR2012のサイトで確認できるものとは異なり、caffeモデルで採用されているラベルIDとなります、従って、Validation dataのGround TruthのIDを、caffeモデルを配布しているサイト(http://dl.caffe.berkeleyvision.org/caffe_ilsvrc12.tar.gz) からダウンロードします。　Validation dataのGround Truthは、`./gt/val.txt`におかれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://dl.caffe.berkeleyvision.org/caffe_ilsvrc12.tar.gz\n",
    "!mkdir gt\n",
    "!tar -xf caffe_ilsvrc12.tar.gz -C ./gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation dataの画像データのダウンロード\n",
    "\n",
    "画像データはILSVRC 2012で配布されているもの(http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_val.tar) を利用します。非常に大きく、ダウンロードに数時間かかるため、適当なところで停止(■を押すなど)することができます。その場合は、ダウンロードできたところまで展開して、`./images`に保存されますが、最後の画像が破損する可能性がありますので削除してください（Last modifiedの新しいファイルが破損している可能性が高いです）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_val.tar\n",
    "!tar -xf ILSVRC2012_img_val.tar\n",
    "!mkdir images\n",
    "!mv *.JPEG ./images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習済みモデルのロード\n",
    "学習済みモデルのロードの手順は以下のとおりです。\n",
    "1. 入力のテンソルを定義します。\n",
    "2. `mobilenet_v1.mobilenet_v1`を利用してモデルを定義します。`mobilenet_v1`はpythonファイルをダウンロードして使用します。\n",
    "3. ckptファイルを読み込んで、学習済みのパラメータで初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "from  tensorflow.contrib.slim import nets\n",
    "checkpoint_file = 'mobilenet_v1_0.25_128.ckpt'\n",
    "\n",
    "# In order to use MobileNet v1, we expoloit mobilenet_v1.py in the official TF repo.\n",
    "# This helps defining mobilenet network.\n",
    "download_file = \"mobilenet_v1.py\"\n",
    "if not os.path.exists(download_file):\n",
    "    print('Download mobilenet_v1.py')\n",
    "    url =\"https://raw.githubusercontent.com/tensorflow/models/master/research/slim/nets/mobilenet_v1.py\"\n",
    "    urllib.request.urlretrieve(url, download_file)\n",
    "\n",
    "# Initialize the network.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Defining the network and load checkpoint\n",
    "input_tensor = tf.placeholder(tf.float32, shape=(None,128,128,3), name='input_image')\n",
    "sess = tf.Session()\n",
    "import mobilenet_v1\n",
    "with tf.contrib.slim.arg_scope(mobilenet_v1.mobilenet_v1_arg_scope()):\n",
    "    logits, end_points = mobilenet_v1.mobilenet_v1(inputs=input_tensor, depth_multiplier=0.25, is_training=False, num_classes = 1001)\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像を読み込んでテストしてみます。表示されるラベルは、ダウンロードしたGround Truthのラベルよりも1多いことに注意してください（Mobilenetでは背景IDが最初に追加されているためです）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_img(pil_image, height, width):\n",
    "    image = pil_image.resize((height,width)).convert('RGB')\n",
    "    image = (np.array(image)/255 - 0.5 )*2\n",
    "    return image.reshape(-1, 128,128,3)\n",
    "\n",
    "height, width = 128, 128\n",
    "im = Image.open('images/ILSVRC2012_val_00001545.JPEG')\n",
    "im = standardize_img(im, height, width)\n",
    "prob = sess.run(end_points['Predictions'], feed_dict={input_tensor: im})\n",
    "prob = prob.reshape(-1)\n",
    "\n",
    "# top-5 labels (but not sorted)\n",
    "ind = np.argpartition(prob, -5)[-5:]\n",
    "\n",
    "# top-5 labels\n",
    "# Taking negtive (-) yields descending order\n",
    "sorted_class = ind[np.argsort(-prob[ind])]\n",
    "print(sorted_class - 1) # The first class_id is background, which should be removed\n",
    "print(prob[sorted_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow serving 用のモデルにexportしてS3にアップロード\n",
    "NeoでコンパイルするモデルはS3にアップロードされている必要があります。\n",
    "- `tf.saved_model.simple_save`を利用してモデルを保存し、model.tar.gzに圧縮します。\n",
    "- SageMaker Python SDKを利用して、model.tar.gzをアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "export_path = \"./export\"\n",
    "if os.path.exists(export_path):\n",
    "    shutil.rmtree(export_path)\n",
    "\n",
    "tf.saved_model.simple_save(\n",
    "    sess,\n",
    "    os.path.join('export/Servo/1/'),\n",
    "    inputs={'input_image': input_tensor},\n",
    "    outputs={'output': end_points['Predictions']})\n",
    "\n",
    "!tar -zcvf model.tar.gz export\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix_name  = 'pretrained_model/resnet_tf'\n",
    "file_name = 'model.tar.gz'\n",
    "model_file = sagemaker_session.upload_data(path=file_name, bucket=bucket_name, key_prefix=prefix_name)\n",
    "print('Your pretrained model is uploaded to: {}'.format(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルのデプロイ\n",
    "ここでは、コンパイルしないモデルとコンパイルしたモデルの双方をデプロイしてみます。\n",
    "\n",
    "### コンパイルしないモデルのデプロイ\n",
    "`TensorFlowModel`に、先ほどアップロードしたS3のモデルのパス`model_file`を与えてモデルを読み込みます。entry pointとして`mobilenet_neo.py`を読み込んでいますが、コンパイル無しのモデルのみをデプロイする場合、特にエントリーポイントを利用しません。ただし、entry_pointは必須の引数なので任意のファイルを指定する必要があります。今回は、このentry pointを後のコンパイルしたモデル用に利用しますので、ここで指定しておき、あとの処理に引き継ぎます。モデルを読み込んだら`deploy`をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import tensorflow\n",
    "from datetime import datetime\n",
    "\n",
    "tf_model = sagemaker.tensorflow.model.TensorFlowModel(model_data = model_file,\n",
    "                                            entry_point ='mobilenet_neo.py', \n",
    "                                            py_version='py2',\n",
    "                                            framework_version='1.11.0',\n",
    "                                            sagemaker_session=sagemaker_session)\n",
    "predictor = tf_model.deploy(initial_instance_count = 1,instance_type = 'ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コンパイルしたモデルのデプロイ\n",
    "\n",
    "モデルのコンパイルでは、`input_shape`に最初に定義した入力テンソルの名前とshapeを指定する必要があります。またoutput_pathも指定します。`compile`関数でモデルをコンパイルできます。コンパイルが終われば、モデル名やイメージ名を指定してデプロイします。\n",
    "\n",
    "デプロイによって作成されたエンドポイントは、png形式で画像を受け取り、エントリーポイントでnumpy形式に変換されて推論をします。png形式で画像をうけとるための`png_serializer`を定義して設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_compiled_model = tf_model.compile(target_instance_family='ml_c5', \n",
    "                                     input_shape={'input_image': [1,128,128,3]},\n",
    "                                     output_path = 's3://'+sagemaker_session.default_bucket() + '/compiled-model/tf-mobilenet/',\n",
    "                                     role = role,\n",
    "                                     job_name =\"tensorflow-resnet-neo-compile-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n",
    "                                     framework='tensorflow',\n",
    "                                     framework_version='1.11.0')\n",
    "tf_compiled_model.name = \"tensorflow-neo-compile-model-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "tf_compiled_model.image = '301217895009.dkr.ecr.us-west-2.amazonaws.com/sagemaker-neo-tensorflow:1.11.0-cpu-py3'\n",
    "compiled_predictor = tf_compiled_model.deploy(initial_instance_count = 1,instance_type = 'ml.c5.xlarge')\n",
    "import io\n",
    "def png_serializer(data):\n",
    "    im = Image.fromarray(data.reshape((128,128,3)))\n",
    "    f = io.BytesIO()\n",
    "    im.save(f, format='png')\n",
    "    f.seek(0)\n",
    "    return f.read()\n",
    "\n",
    "compiled_predictor.content_type = 'application/x-image'\n",
    "compiled_predictor.serializer = png_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コンパイルしたモデルをElastic inferenceでデプロイ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_compiled_model.name = \"tensorflow-neo-compile-model-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "eia_compiled_predictor = tf_compiled_model.deploy(initial_instance_count = 1,instance_type = 'ml.c5.xlarge', accelerator_type='ml.eia1.medium')\n",
    "eia_compiled_predictor.content_type = 'application/x-image'\n",
    "eia_compiled_predictor.serializer = png_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論性能の比較\n",
    "\n",
    "### 推論時間の比較\n",
    "`./images`にある画像から100枚の画像をランダムに抽出して、以下の３種類のエンドポイントで推論します。\n",
    "\n",
    "- コンパイルしていないモデル \n",
    "- コンパイルしたモデル\n",
    "- コンパイルしたモデルとElastic Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "image_files = os.listdir(\"./images\")\n",
    "index = np.random.choice(len(image_files), 100, replace=False)\n",
    "image_files = [image_files[i] for i in range(len(image_files)) if i in index]\n",
    "\n",
    "baseline_time = 0\n",
    "compile_time = 0\n",
    "eia_compile_time = 0\n",
    "print(\"Evaluating three endopoints with 100 images.\")\n",
    "for f in tqdm(image_files):\n",
    "    pil_img = Image.open('images/'+f)\n",
    "\n",
    "    #Preprocess for base model\n",
    "    image = pil_img.resize((128,128)).convert('RGB')\n",
    "    image = (np.array(image)/255 - 0.5 )*2\n",
    "    im =  image.reshape(-1, 128,128,3)\n",
    "    \n",
    "    #Inference by base model\n",
    "    start = time.time()\n",
    "    result = predictor.predict({'input_image':im})\n",
    "    baseline_time += time.time()-start\n",
    "\n",
    "    #Preprocess for compiled  model\n",
    "    im = np.array(pil_img.resize((128,128)).convert('RGB'))\n",
    "    \n",
    "    #Inference by compiled model\n",
    "    start = time.time()\n",
    "    predict_response = compiled_predictor.predict(im)\n",
    "    compile_time += time.time()-start\n",
    "\n",
    "    #Inference by compiled model with EI\n",
    "    start = time.time()\n",
    "    predict_response = eia_compiled_predictor.predict(im)\n",
    "    eia_compile_time += time.time()-start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.size\"] = 18\n",
    "\n",
    "left = np.array([1, 2, 3])\n",
    "height = np.array([baseline_time/100.0,  compile_time/100.0, eia_compile_time/100.0])\n",
    "label = [\"Base model\", \"Compiled\", \"Compiled w/ EI\"]\n",
    "plt.bar(left, height, tick_label=label, width=0.5,linewidth=2, color=\"#00FF7F\", edgecolor=\"#20B2AA\")\n",
    "\n",
    "plt.ylabel(\"Time [seconds]\")\n",
    "plt.xlabel(\"Deployed model\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df = pd.DataFrame(height, index = label, columns=['Time [ seconds]'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メモリ使用量の比較\n",
    "\n",
    "boto3 を利用してCloudWatchのメトリクスを見てみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import boto3\n",
    "from boto3.session import Session\n",
    "import datetime\n",
    "\n",
    "response = client.get_metric_data(\n",
    "    MetricDataQueries=[\n",
    "        {\n",
    "            'Id': 'base',\n",
    "            'MetricStat': {\n",
    "                'Metric': {\n",
    "                    'Namespace': '/aws/sagemaker/Endpoints',\n",
    "                    'MetricName': 'MemoryUtilization',\n",
    "                    'Dimensions': [\n",
    "                        {\n",
    "                            'Name': 'EndpointName',\n",
    "                            'Value': predictor.endpoint\n",
    "                        },\n",
    "                        {\n",
    "                            'Name': 'VariantName',\n",
    "                            'Value': 'AllTraffic',\n",
    "                        },\n",
    "                    ]\n",
    "                },\n",
    "                'Period': 1,\n",
    "                'Stat': 'Average',\n",
    "            },\n",
    "            'ReturnData': True\n",
    "        },\n",
    "        {\n",
    "            'Id': 'compiled',\n",
    "            'MetricStat': {\n",
    "                'Metric': {\n",
    "                    'Namespace': '/aws/sagemaker/Endpoints',\n",
    "                    'MetricName': 'MemoryUtilization',\n",
    "                    'Dimensions': [\n",
    "                        {\n",
    "                            'Name': 'EndpointName',\n",
    "                            'Value': compiled_predictor.endpoint\n",
    "                        },\n",
    "                        {\n",
    "                            'Name': 'VariantName',\n",
    "                            'Value': 'AllTraffic',\n",
    "                        },\n",
    "                    ]\n",
    "                },\n",
    "                'Period': 1,\n",
    "                'Stat': 'Average',\n",
    "            },\n",
    "            'ReturnData': True\n",
    "        },\n",
    "        {\n",
    "            'Id': 'compiledei',\n",
    "            'MetricStat': {\n",
    "                'Metric': {\n",
    "                    'Namespace': '/aws/sagemaker/Endpoints',\n",
    "                    'MetricName': 'MemoryUtilization',\n",
    "                    'Dimensions': [\n",
    "                        {\n",
    "                            'Name': 'EndpointName',\n",
    "                            'Value': eia_compiled_predictor.endpoint\n",
    "                        },\n",
    "                        {\n",
    "                            'Name': 'VariantName',\n",
    "                            'Value': 'AllTraffic',\n",
    "                        },\n",
    "                    ]\n",
    "                },\n",
    "                'Period': 1,\n",
    "                'Stat': 'Average',\n",
    "            },\n",
    "            'ReturnData': True\n",
    "        },\n",
    "    ],\n",
    "    StartTime=datetime.datetime.utcnow() - datetime.timedelta(seconds=1200),\n",
    "    EndTime=datetime.datetime.utcnow(),\n",
    "    ScanBy='TimestampAscending',\n",
    "    MaxDatapoints=20000\n",
    ")\n",
    "\n",
    "results = response['MetricDataResults']\n",
    "for r in results:\n",
    "    if r['Id'] == 'base':\n",
    "        base_memory = r['Values'][:10]\n",
    "    elif r['Id'] == 'compiled':\n",
    "        compiled_memory = r['Values'][:10]\n",
    "    elif r['Id'] == 'compiledei':\n",
    "        compiled_ei_memory = r['Values'][:10]\n",
    "\n",
    "x_label  = range(10)\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.rcParams[\"font.size\"] = 18\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "plt.title(r\"Base model\")\n",
    "ax1.plot(x_label, base_memory)\n",
    "ax2 = fig.add_subplot(1, 3, 2)\n",
    "plt.title(r\"Compiled model\")\n",
    "ax2.plot(x_label, compiled_memory)\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "plt.title(r\"Compiled model with EI\")\n",
    "ax3.plot(x_label, compiled_ei_memory)\n",
    "plt.show()\n",
    "fig.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
